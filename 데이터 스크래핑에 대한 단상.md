데이터 스크래핑, 소위 "스크래핑 땔깜"과 관련된 내용들을 정리해보고 싶어서 간만에 글을 작성해본다.

## 데이터 수집과 그 이면

많은 데이터를 가지는 것은 늘 중요하다. 하지만 정작 그 프로세스에는 관심을 가지는 이들이 드물다.

대기업은 이와 관련하여 대규모의 자사 프로덕트를 가지고 있고, 그로부터 파생된 많은 기초 데이터를 가지고 있으며, 이를 적재하거나 처리하는 부분에만 고민을 하는 경우가 많다.
<br>
그렇기에 많은 개발자 채용들 또한 `ETL`, `ELT`, `데이터 레이크`, `Hadoop`, `Spark`, `Airflow` 등에 대한 지식을 요구한다.

반면 자사 프로덕트가 없는 중소기업 이하의 회사들은 이에 대해 아무리 고민을 한들 해결할 수 있는 것이 없다.
<br>
그저 "당장 데이터가 필요하니까" 라는 이유로, 데이터 수집을 빙자한 스크래핑에 불과한 개발을 진행하는 경우가 많다.
<br>
**이렇게 주먹구구식으로 개발된 데이터 수집기들은 경험해 본 바로는 거의 전부 유지보수가 되지 않는다.**

## 열악함에 대하여

많은 회사들이 데이터 수집이 중요하다고 이야기하면서, 정작 그 프로세스의 적법성, 효율성, 정확성에는 언급을 하지 않는다.

개인적으로 가장 심각하다고 생각되는 부분들은 다음과 같다.

- 적법성
    - **불법적으로 데이터를 수집하는 경우가 많다.**
        - **프록시를 사용하여 매우 공격적으로 수집하는 경우가 많다.** 
            - 수집 대상 사이트에게는 사실상 DDOS 공격을 하는 것과 다름이 없다.
        - 경쟁사 데이터를 수집하는 경우도 있다.
    - 공식적인 API를 제공하더라도, **속도가 더 빠르다면 비공식 API를 사용해서 데이터를 수집한다.**


- 효율성
    - **대부분의 코드에서 유지보수가 되지 않는다.**
    - **데이터 수집을 위한 코드는 누구나 작성할 수 있으니까, 누구나 유지보수를 할 수 있다고 생각한다.**
        1. 회사에서는 프로그램이 돌아만 가면 상관 없게 여긴다.
        2. 개발자들은 자신이 사용하는 개발 언어에 대해 이해하고 있지도 않고, 유지보수를 할 생각도 없다.
        3. 기술자 대우를 우습게 여기니, 기술자들도 현재 개발 중인 코드에 대해 책임감을 가지지 않는다.
        4. 결국 스파게티 코드, 더러운 코드가 쌓이게 된다.
        5. 신규 개발자는 절대로 이 코드를 이해할 수 없다.
        6. 유지보수 측면에서 최악이며, 하나의 공통 로직을 고치려 해도 수십, 수백 개의 수집기 로직을 고치는 경우가 많다.
    - 코드의 언어, 프레임워크, 라이브러리, CPU / 메모리 사용량 등에 대한 고민이 없다.
        - 핑거프린팅 관련 기능이나 CSR이 없지만, 자바나 C언어로 무려 헤드리스 브라우저를 사용해서 수집한다.
            - **이는 개발 지식 부족에서 기인한다.**
                - 몰라서 사용하는 것은 전혀 문제가 없지만, **알고도 사용한다면... 정말 큰 문제다.**
            - 최악을 선택한다.
                - **Playwright, Puppeteer 등을 선택할 수 있음에도 꼭 Selenium을 선택한다.**
                - 사용할 줄 아는 개발자가 더 많다는 이유 때문이 많았다.
                - **그리고 Selenium을 사용한 프로덕트들은 반드시 문제를 겪었다.**
        - 자신들이 편하다는 이유로 장벽이 높은 방법을 택하는 경우도 있다.
            - 쉘 스크립트, C언어,
        - 기초적인 개발자로서의 지식이 없다.
            - 수백만, 수천만 건의 중복 체크에 Hash 기반 자료 구조를 사용하지 않고 리스트를 사용하는 경우도 본 적 있다.
            - 신입 개발자면 그럴 수 있지만 수 년차 개발자가 그런다면... 정말 큰 문제다.

- 정확성
    - 테스트 코드가 대부분 없다.
        - 테스트 코드가 없는 것 자체로 문제가 되지는 않는다고 생각한다.
            - 특히, 자사 프로덕트가 아니고 빠른 주기로 큰 변경이 있다면, 테스트 코드가 없는 것은 문제가 아닐 수도 있다.
            - **하지만 타사 프로덕트에 변경점도 적다면, 최소한 데이터 추출 로직에 대해서는 테스트 코드가 필요하다.**
        - 자사 프로덕트임에도 불구하고 제대로 설계하지 않고서 대충 `Sentry` 로 떼우는 경우가 있다.
            - `Sentry`에서 원인 분석 하는데도 한참 걸린다.
            - 개발자가 `Comment` 창 들어가서 댓글 달고, `Resolve` 버튼 눌러주는 비용은 무료가 아니다.
        - **비즈니스적으로 중요한 부분에 대해서도 테스트 코드가 필요하다.**
            - 데이터 수집 건 수, 전 수집과 비교, 평균 수집량, 수집 시간, 수집 실패 건 수 등 기초적인 수집 관련 데이터 등
            - 수집된 데이터의 정합성, 중복 여부 등
            - 데이터 전처리 등
    - **도메인, 비즈니스적으로 정확한 내용을 수집하지 않는 경우가 많다.**
        - RDB의 JSON 컬럼, 또는 MongoDB와 같은 NoSQL에 그냥 적재하고 생각하는 경우가 많았다.
            - 소위 ELT 방식의 선 적재가 문제가 되는 것이 아니라, **그냥 아무도 도메인 분석을 안한다는 뜻이다.**
    - **이상 탐지 기준이 없다.**
        - **문제가 생기더라도 한참 뒤에 알아차릴 수 있다.**
        - 로그에서 에러 문자열을 찾아서 이를 이상 탐지 기준으로 삼는 경우도 있다.
            - 당신의 회사에서 이렇게 하고 있다면, 모니터링 담당자는 퇴사를 심각하게 고민하고 있다.

## 프레임워크

데이터 크롤링, 스크래핑 프레임워크로 유명한 프레임워크들을 나열해보면 다음과 같다.

- `Scrapy`
- `Gocolly`
- `Crawlee`
- `Webmagic`
- `Crawler4j`

이 중에 개발 전담 회사가 있는 프레임워크는 단 두 개 (`Scrapy`, `Crawlee`)이며, `Crawlee` 도 그다지 좋은 모습을 보여주지 못한다.

그나마 상위 3개 프레임워크에 대해 이야기를 해보자면,

- `Scrapy`
    - 장점
        - **현재로서는 그나마 비즈니스적으로 가장 안정적인 프레임워크이다.**
        - Django 에서 많은 영향을 받아서 그런지, 확장이라는 측면에서 그나마 유일하게 개방적이다.
    - 단점
        - Twisted 라이브러리를 사용하고 있는데, 난이도가 높다.
            - Python의 고급 문법들을 복잡하게 사용하고 있어, **Core 로직을 손 댈 것이 아니면 이해하기도 번거롭다.**
        - Python 언어의 특성상 단일 프로세스만을 사용할 가능성이 높다.
            - **컴퓨팅 자원을 비효율적으로 사용하게 된다.**
        - 개선이 오래 걸린다.
            - **프로젝트가 커져서 코어 로직에 손 대기가 매우 어려워졌다.**
        - 장기간 방치된 깃허브 이슈들이 많다.

- `Gocolly`
    - 장점
        - 빠르다.
        - 텍스트를 기반으로 한다면, 타 수집기보다 기본적으로 지원하는 기능이 많다.
    - 단점
        - 유지보수가 되지 않는다.
        - 수집기를 실행하는 측면이 타 수집기에 비해 상당히 번거롭다.
            - 래핑에 대한 고민을 개발자가 많이 신경 써야 한다.

- `Crawlee`
    - 장점
        - 기능이 많다.
        - 유지보수가 잘 되고 있다.
        - JS 기반이고 자원을 효율적으로 사용할 수 있다.
            - 심지어 이를 자동적으로 처리해주는 기능도 많다.
    - 단점
        - 말은 그럴싸 하지만, 본인들이 운용 중인 `Apify` 플랫폼에 대한 은근한 끼워팔기로 느껴질 때가 많다.
            - `Scrapy` 는 본인들이 `Zyte`라는 회사에서 유지보수를 하고 있지만, `Zyte` 사의 서비스를 이용하지 않는다는 이유로 불편한 부분은 없다.
            - 반면 `Crawlee`는 `Apify` 라는 서비스에 과도적으로 의존하는 모습을 보일 때가 많다.
                - `Apify` 를 이용하지 않으면 개발이 어려운 경우도 있다.
        - 더 잘 만들어진 프레임워크인 척 하지만, 실상은 headless browser 와 proxy에만 기반한 기능들을 주로 내세운다.
            - 실제로는 크롤링/스크래핑에 필요한 기능들을 제공하지 않는다.
            - 그저 headless browser를 사용하여 타겟 사이트를 속이는 일에만 집중하는 것으로 느껴질 때가 많다.
                - 어떤 단어에 대한 의미를 따지는 것을 좋아하지는 않지만... 이건 "크롤러" 가 아니다. "스크래퍼" 로서도 나쁘다.
            - 타 언어, 프레임워크, 라이브러리는 이런 기능을 제공 못하는 것도 아니다.

## 개선 방향

관련 개발자로서 생각하는 조그마한 개선 방향은 다음과 같다.

- **고효율** 모니터링
    - 시스템적으로 모니터링 도구를 도입하여 능률을 올려야 한다.
        - `Sentry`, `Prometheus`, `Grafana` 등을 도입해야 한다.
        - 클라우드 환경에서 지원하는 모니터링 도구도 사용해야 한다.
            - `AWS CloudWatch` 등
    - 직접 개발, 보완하는 모니터링 기능도 필요하다.
        - Slack 등 메신저 연동 기능
        - 프로덕트 내 도메인 관련 모니터링
        - 이를 빠르고 쉽게 개발해서 프로덕트에 가져다 붙일 수 있어야 한다.
    - 비개발자가 모니터링이 가능할 정도로 알림 기능이 갖추어져야 한다.
- 워크플로우 관리 도구 도입
    - 전반적인 프로세스에서 어디에 문제가 있는지를 직관적으로 알아차릴 수 있어야 한다.
    - `Airflow` 가 나온지도 10년이 되어간다.
- CI/CD
    - **반드시 자동화를 해야한다.**
    - **제발 최소한 Github Action(workflow) 을 쓰자.**
        - 대부분의 소스 코드 저장소에서 관련 기능을 제공한다.

## 결론

- 소위 말하는, "뇌 빼고도 개발이 가능한 구조"를 만드는 것이 중요하다고 생각한다.
    - 문제가 어딘지 즉각적으로 알 수 있는 구조를 만들어야 한다.
- 오래된 프레임워크들에 많이 의존을 해왔지만, 이제 신식 문물을 적용할 때가 왔다.
